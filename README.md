Project Context Prompt â€” file_classifier

You are helping me maintain and extend a Python data-engineering project called file_classifier.

The project is already implemented, stable, and working end-to-end.
Your job is to extend or refactor it without breaking its guarantees, performance, or mental model.

ğŸ¯ Core Goal

Automatically classify messy spreadsheet files (XLSX, CSV) into business-meaningful categories using schema detection, not filenames or folders.

The system must remain:

deterministic

auditable

fast

explicit about unknown schemas

manually controlled for semantic meaning

ğŸ§  Core Philosophy

Headers define meaning

Formats do not

Filenames lie

A fileâ€™s schema (exact normalized header set) is its identity.

ğŸ“¥ Input Assumptions

Input files live in datalake/

Input is read-only

Supported formats:

.xlsx

.csv

Each file:

Has a single logical table

May include metadata rows above the real header

May have broken Excel metadata (UsedRange issues)

ğŸ—‚ï¸ High-Level Architecture
datalake/              # raw input (never touched)
   â†“
pipeline
   â†“
data/staging/          # metadata, audits, decisions (truth)
   â†“
data/classified/       # business snapshot (gold)

ğŸ—ƒï¸ Folder Structure
file_classifier/
â”œâ”€â”€ datalake/                  # raw input files (gitignored)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ staging/               # audit & metadata (Parquet)
â”‚   â”‚   â”œâ”€â”€ file_catalog.parquet
â”‚   â”‚   â”œâ”€â”€ schema_registry.parquet
â”‚   â”‚   â””â”€â”€ classification_manifest.parquet
â”‚   â”‚
â”‚   â”œâ”€â”€ classified/            # snapshot output (gold)
â”‚   â”‚   â”œâ”€â”€ <label>/
â”‚   â”‚   â””â”€â”€ unknown_schema/
â”‚   â”‚
â”‚   â””â”€â”€ quarantine/            # unreadable / low-confidence files
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                # orchestrator
â”‚   â”‚
â”‚   â”œâ”€â”€ io/
â”‚   â”‚   â”œâ”€â”€ scanner.py         # recursive file discovery
â”‚   â”‚   â””â”€â”€ preview_reader.py  # XLSX + CSV preview readers
â”‚   â”‚
â”‚   â”œâ”€â”€ fingerprint/
â”‚   â”‚   â”œâ”€â”€ header_detector.py # robust header detection
â”‚   â”‚   â””â”€â”€ header_normalizer.py
â”‚   â”‚
â”‚   â”œâ”€â”€ classify/
â”‚   â”‚   â””â”€â”€ file_copier.py     # snapshot-safe copy logic
â”‚   â”‚
â”‚   â””â”€â”€ labeling/
â”‚       â””â”€â”€ schema_labels.py   # schema_hash â†’ label loader
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings.yaml
â”‚   â”œâ”€â”€ header_aliases.yaml
â”‚   â””â”€â”€ schema_labels.yaml     # manual schema â†’ label mapping
â”‚
â””â”€â”€ README.md

ğŸ” Pipeline (Step by Step)
1ï¸âƒ£ Scan

Recursively scan datalake/

Collect:

path

size

modified time

Sorted for deterministic runs

2ï¸âƒ£ Preview Read (cheap)

Read only first N rows

XLSX: pandas.read_excel(nrows=N)

CSV: pandas.read_csv(nrows=N, sep=None)

Avoid full reads

3ï¸âƒ£ Header Detection (Heuristic)

Candidate rows scored by:

non-empty density

text vs numeric ratio

short-string dominance

uniqueness

coherence with following rows

If confidence < threshold â†’ low_confidence

4ï¸âƒ£ Header Normalization

Rules:

lowercase

trim

remove accents

spaces/dots/dashes â†’ _

collapse _

remove non [a-z0-9_]

deduplicate (col, col__2, â€¦)

optional alias mapping (header_aliases.yaml)

5ï¸âƒ£ Schema Identity

Schema = set of normalized headers (order ignored)

schema_key = sorted join (audit)

schema_hash = sha1(schema_key)[:12] (stable identity)

6ï¸âƒ£ Manual Semantic Labeling

Load config/schema_labels.yaml

Map:

schema_hash â†’ business label


Missing â†’ unknown_schema

No auto-labeling ever

7ï¸âƒ£ Persist Staging Artifacts (Truth Layer)
file_catalog.parquet

One row per file:

path

size

modified_ts

status (ok | unreadable | low_confidence)

header_row_index

header_confidence

raw_headers_json

normalized_headers_json

schema_key

schema_hash

schema_id

label

schema_registry.parquet

One row per schema:

schema_id

schema_hash

schema_key

canonical_headers_json

file_count

example_files_json

classification_manifest.parquet

One row per copy attempt:

src_path

dst_path

copy_status

error_message

schema_id

schema_hash

label

8ï¸âƒ£ Physical Classification (Gold Snapshot)

Snapshot mode:

Gold folders are wiped per label per run

No accumulation

Output:

data/classified/<label>/<schema_hash>__original_filename.ext


Unreadable / low confidence â†’ data/quarantine/

ğŸ§  Key Mental Model
Layer	Purpose
datalake/	raw input
staging/	truth & audit
classified/	business snapshot
schema_hash	identity
label	meaning

Staging answers â€œwhyâ€.
Classified answers â€œwhereâ€.

ğŸ§¾ CLI Output (psql-style)

The CLI shows a compact table:

schema                     files  headers  rows
wellsky_clients                6       18      6
ringcentral_calls              8       10      8
...


files = number of files in snapshot

rows = number of files (cheap, not data rows)

ğŸš¨ Unknown Schema Handling

Unknown schemas:

go to classified/unknown_schema/

printed in console

User manually updates schema_labels.yaml

Next run â†’ auto-classified

This is intentional and required.

â–¶ï¸ How to Run
python -m src.main


Dry run:

python -m src.main --dry-run

ğŸ§± Current State (Important)

End-to-end stable

XLSX + CSV supported

Snapshot logic working

CLI clean and fast

Performance optimized

No accumulation

No silent behavior

ğŸš« Hard Constraints for All Future Work

âŒ Do not bypass staging

âŒ Do not auto-label schemas

âŒ Do not break snapshot semantics

âŒ Do not re-read full files unnecessarily

âŒ Do not mix audit and presentation layers

ğŸš€ Approved Future Extensions (Optional)

Incremental processing via file_sig

Persisted real row counts (computed once)

Archive historical snapshots

CLI filters (--only unknown, --only label X)

CSV dialect overrides

Final Instruction to Assistant

Preserve the mental model first,
then optimize or extend without regressions.